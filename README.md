# FMCW_Radar_Moco
This project is an attempt to apply an improved MoCo model to the task of FMCW radar-based gesture recognition. Furthermore, because the number of negative samples in contrastive learning significantly affects model performance, we adopted a  Gradcache technique. This allowed us to train with a large batch size of 1024 on a single A30 GPU, effectively addressing the challenge of limited negative samples.

## GradCache
GradCache is a technique that enables the use of large batch sizes in contrastive learning by caching gradients from previous iterations. This approach allows for efficient training without requiring excessive memory resources, making it particularly useful in scenarios with limited computational power.<br>

Github repository: [GradCache](https://github.com/luyug/GradCache)<br>
Paper Link: [Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup](https://arxiv.org/abs/2101.06983)<br>


## File structure

The project's documentation is organized as follows: 

```
Gesture_dataset				              
    |------data
    |------labels
Logs	                              
    |------train.log
    |------test.log
    |------model.pth                 
 

.gitignore
README.md
run.py                              --Main file for training and testing
dataset.py                          --Dataset class for loading and processing data
encoder.py                          --Encoder class for the MoCo model
PatchEmbedding.py                   --Patch embedding class for the MoCo model
moco_framework.py                   --MoCo framework class for contrastive learning
train_epoch.py                      --Training epoch class for the MoCo model
Feature_extractor_1.ipynb           --Feature extractor test notebook
Feature_extractor_2.ipynb           --Feature extractor test notebook
transformer_block.py                --Transformer block class for the MoCo model
```

## Datasets
The dataset utilized in this project is a gesture dataset that includes radar signals (Range-Doppler Images or RDI) and corresponding labels. This data was obtained from Infineon, where I completed both my internship and my master's thesis. The dataset is organized with a `data` folder for the radar signals and a `labels` folder for the corresponding gesture labels.<br>

In this dataset, each radar signal is represented as a Range-Doppler Image (RDI). The RDI visualizes the range and velocity of objects detected by the FMCW radar. The corresponding gesture labels are provided in a separate file. The dataset includes five distinct gestures: `Push`, `SwipeLeft`, `SwipeRight`, `SwipeUp`, and `SwipeDown`.<br>

### Data preprocessing
#### Datas
An RDI is a 2D image representation of a radar signal, with the x-axis representing range and the y-axis representing Doppler frequency. The data is preprocessed to be compatible with the MoCo model.<br>

The shape of the RDI is `(32, 6, 32, 32)`.<br>

32: number of frames<br>

6: number of channels (The three virtual receive channels from the raw data are transformed into complex numbers after Range FFT and Doppler FFT. Splitting the real and imaginary parts results in a total of six channels.)<br>

32: number of chirps<br>

32: number of samples<br>

#### Labels
The labels for the gestures are provided in a separate file, where each gesture is associated with a specific label. The labels are integers representing the different gestures: `Push` (0), `SwipeLeft` (1), `SwipeRight` (2), `SwipeUp` (3), and `SwipeDown` (4). The labels are used to train the MoCo model to recognize and classify the gestures based on the radar signals.<br>

## Implementation details

### Model and methods
The model is based on the MoCo framework, which utilizes a contrastive learning approach to learn representations from the FMCW radar data. The model architecture includes a transformer block for feature extraction and a patch embedding layer to process the radar signals. The GradCache technique is employed to handle large batch sizes efficiently.<br>

### PatchEmbedding
The PatchEmbedding class converts the input radar RDI into patches for processing by the transformer block, enabling the model to effectively capture local patterns. This block uses the timm library to implement the patch embedding layer, specifically `resnet10t.c3_in1k`. To prevent the features from becoming too small, the max-pooling and first convolutional layers have been modified. The primary goal of this block is to extract intra-frame information.<br>

`data: (32, 6, 32, 32) ---> (32, 256)`

### Feature encoder
The FeatureEncoder class encodes the patches generated by the resnet PatchEmbedding block into a feature representation for contrastive learning. It uses a transformer block to process the input and extract meaningful features. This encoder is specifically designed to handle the characteristics of FMCW radar signals, ensuring the learned representations are robust and discriminative.The primary goal of this block is to extract inter-frame information.<br>

#### Random masking
To enhance the model's robustness, a random masking technique is applied to the input data. This involves randomly masking a portion of the input patches during training, forcing the model to learn to predict the missing information based on the remaining context. This technique helps improve the model's generalization capabilities and resilience to noise in the radar signals.<br>

#### Output
use a aggreate function to combine the features from all frames, resulting in a final feature representation of shape. <br>

`data:(32, 256) ---> (256, )`

### Main moco framework
he MoCo framework, implemented in `moco_framework.py`, is a modified MoCo model that omits the traditional queue. It utilizes the GradCache technique to efficiently manage large batch sizes. This approach enables effective contrastive learning, even when the number of negative samples is limited.<br>


## Results

t-SNE visualization of the raw data<br>

  <img src="image\raw.png" />

t-SNE visualization of the embeddings<br>

  <img src="image\embedding.png" />

KNN results<br>

  <img src="image\knn.png" />

## Summary
After pre-training, the features output by the encoder can effectively separate different gestures in the feature space, providing more robust features for downstream tasks.